---
title: '[R] fit polynomial regression in R'
author: "Zhenguo Zhang"
date: '2019-07-12'
slug: r-fit-polynomial-regression-in-r
tags:
- R
- regression
categories:
- R
- Computing
---



<p>In R, we can fit a polynomial model by combinative use two functions <em>poly()</em> and <em>lm()</em>.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>Let’s start with using the R internal data <em>cars</em>:</p>
<pre class="r"><code>data(cars)
x&lt;-cars$speed # use the speed as predictor
y&lt;-cars$dist # use the dist as response</code></pre>
</div>
<div id="fit-the-models" class="section level2">
<h2>Fit the models</h2>
<p>Now, let’s get the polynomials for the variable <em>x</em> using the degree 3:</p>
<pre class="r"><code>x1&lt;-poly(x, 3, raw=TRUE)
x2&lt;-poly(x, 3, raw=FALSE)
# check the resulted polynomials
head(x)</code></pre>
<pre><code>## [1] 4 4 7 7 8 9</code></pre>
<pre class="r"><code>head(x1)</code></pre>
<pre><code>##      1  2   3
## [1,] 4 16  64
## [2,] 4 16  64
## [3,] 7 49 343
## [4,] 7 49 343
## [5,] 8 64 512
## [6,] 9 81 729</code></pre>
<pre class="r"><code>head(x2)</code></pre>
<pre><code>##               1          2           3
## [1,] -0.3079956 0.41625480 -0.35962151
## [2,] -0.3079956 0.41625480 -0.35962151
## [3,] -0.2269442 0.16583013  0.05253037
## [4,] -0.2269442 0.16583013  0.05253037
## [5,] -0.1999270 0.09974267  0.11603440
## [6,] -0.1729098 0.04234892  0.15002916</code></pre>
<p><em>poly()</em> returns a matrix, with the first column corresponds to original data <em>x</em>, second column to the quadratic term <em>x^2</em>, third column to the cubic term <em>x^3</em>, etc.</p>
<p>As you see, there are two ways to generate polynomial data for a variable using <em>poly()</em> in R, depending on the option <em>raw</em> is <em>TRUE</em> or <em>FALSE</em>. When it is <em>TRUE</em>, the polynomials are simply <em>x</em> to the power of respective degrees. When it is <em>FALSE</em>, the polynomials are further normalized to be orthogonal among each other. We will see how different these two ways are in model fitting below.</p>
<pre class="r"><code>m1&lt;-lm(y ~ x1)
m2&lt;-lm(y ~ x2)
# summarize the models
summary(m1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.670  -9.601  -2.231   7.075  44.691 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -19.50505   28.40530  -0.687    0.496
## x11           6.80111    6.80113   1.000    0.323
## x12          -0.34966    0.49988  -0.699    0.488
## x13           0.01025    0.01130   0.907    0.369
## 
## Residual standard error: 15.2 on 46 degrees of freedom
## Multiple R-squared:  0.6732, Adjusted R-squared:  0.6519 
## F-statistic: 31.58 on 3 and 46 DF,  p-value: 3.074e-11</code></pre>
<pre class="r"><code>summary(m2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.670  -9.601  -2.231   7.075  44.691 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42.98       2.15  19.988  &lt; 2e-16 ***
## x21           145.55      15.21   9.573  1.6e-12 ***
## x22            23.00      15.21   1.512    0.137    
## x23            13.80      15.21   0.907    0.369    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.2 on 46 degrees of freedom
## Multiple R-squared:  0.6732, Adjusted R-squared:  0.6519 
## F-statistic: 31.58 on 3 and 46 DF,  p-value: 3.074e-11</code></pre>
<p>As shown above, the two models are overall the same, both having P value = 3.074e-11, F-stat=31.58, and adjusted R-squared=0.6519. However, the coefficients of the predictors (x, x^2, and x^3) are different, as expected. More importantly, for the model <em>m1</em>, none of the coefficients are significant, while for <em>m2</em>, the coefficient of <em>x</em> is significant. This can be explained as: in model <em>m1</em>, the predictors are highly correlated, so the variance of the coefficients is much larger than that in model <em>m2</em> whose predictors are orthogonal.</p>
</div>
<div id="predict-with-new-data" class="section level2">
<h2>Predict with new data</h2>
<p>Let’s see how to make predictions using the fitted models.</p>
<pre class="r"><code>newx&lt;-seq(min(x),max(x),length.out = 100)
# convert the newx to polynomials first
newx1&lt;-predict(x1, newdata = newx) # here x1 is also &quot;poly&quot; class object
newx2&lt;-predict(x2, newdata = newx)
# then make predictions using the converted data
y1&lt;-predict(m1, newdata = list(x1=newx1)) # the newdata follows the format of the original model fitting
y2&lt;-predict(m2, newdata = list(x2=newx2))</code></pre>
<p>As you see in the plots below, the predictions from the two models are essentially the same and fit the original data very well.</p>
<pre class="r"><code># compare y1 and y2
plot(y1,y2, cex=0.6, col=&quot;blue&quot;, cex.lab=0.8, cex.axis=0.7, main=&quot;Compare predictions from two models&quot;)
abline(0,1,col=&quot;black&quot;)</code></pre>
<p><img src="/post/2019-07-12-r-fit-polynomial-regression-in-r_files/figure-html/plots-1.png" width="672" /></p>
<pre class="r"><code># also check the fit of the prediction with the original data
plot(x,y, cex=0.6, cex.lab=0.8, cex.axis=0.7, main=&quot;model fit on original data&quot;)
lines(newx,y1,col=&quot;red&quot;)
legend(&quot;topleft&quot;,legend=&quot;prediction&quot;, lty=&quot;solid&quot;, col=&quot;red&quot;, cex=0.8)</code></pre>
<p><img src="/post/2019-07-12-r-fit-polynomial-regression-in-r_files/figure-html/plots-2.png" width="672" /></p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<ol style="list-style-type: decimal">
<li><p>one can use <em>poly()</em> and <em>lm()</em> to fit polynomial models in R.</p></li>
<li><p>actually, one can directly specify <em>lm(y ~ poly(x, degree))</em> to fit models. In this case, the new data should also be given as raw x without any polynomial conversion.</p></li>
<li><p>raw and orthogonal transformed polynomials make difference only in the term cofficients of models, but have no effect on the predictions.</p></li>
</ol>
</div>
